## Multi-Armed Bandit (MAB) Experiment

#### Overview

This project implements and compares two popular multi-armed bandit (MAB) algorithms:

Epsilon-Greedy with Exponential Decay

Thompson Sampling using Beta Distribution

The goal is to analyze how these algorithms explore and exploit different choices to maximize cumulative rewards while minimizing regret.

Also, you can find better ideas to implement these algorithms at the end.
